# Single Line Comment

"""
Multiple
Line 
Comment
"""

# Python is a package driven language
"""
TO install new packages
Open anaconda prompt and type the following

conda install package_name
conda install seaborn
"""

"""
To update existing packages
Open anaconda prompt and type the following

conda update package_name
conda update spyder
"""

# F9: Runs a line and moves the cursor to next line
# Cntrl + Enter: Run selected line(s)
# F5: Runs the entire script. Only print/warning/error outputs will be shown in consile
a = 2
b = 3
#print(f)
c = a*b
print(c)

6/5 # will NOT be printed in run time
4*3 # will NOT be printed in run time
print(4*3) # will be printed in run time

del(b)

## Python is case sensitive
f = 6
F = 7

#Print(f)

## ARITHMETIC OPERATORS
6 + 3 # addition
6 - 3 # subtraction
6 * 3 # multiplication
5 / 2 # division
5 // 2 # quotient
5 % 2 # remainder

## user inputs

ip1 = input("Enter input 1: ")
ip2 = input("Enter input 2: ")
print(ip1, ip2, sep = "|")

###################################### Data Types ##################
# ()
  # passing input to functions
  # creating tuples

# []
  # Slicing/Extracting
  # Creating a list
# {}
  # Creating dictionary in {Key:Value} pair format
  # Creating set 

################ Basic/Atomic Data types in Python ##################

# int, float, str, bool, complex

##################### integer #################################
## any number without decimal points is treated as integer
a = 2
type(a)
b = -10
c = 123444444444444444444666666666666677777777777777 # no long int

################## float ###############################
# any number with decimal point is a float
d = 123.4556
type(d)

## float to integer and vice versa
b_f = float(b)
d_i = int(d) # nearest small integer
int(6.7)
round(6.4)
round(6.6)

############## String ##############################################
# anything with quotes is string
s = "nature"
type(s)
s1 = 'nature' # even single quotes is a string
s2 = "999" # even a number within quotes is a string
s3 = "n" # there is no character data type. even a single character is string

s2_i = int(s2) # string to integer
#int(s) throws error as nature cannopt be converted to an integer

#g = 05 throws error
g = "05"
int(g) # works.. 

5 + 10 # addition
"Hello" + "World" # concatenation

########### Boolean ################################################
h = True
type(h)
i = False
#j = true throws error as it is case sensitive

cond1 = 5 > 6
cond2 = 6 > 5
cond3 = 6 == 5 # equality check
cond4 = 6 != 5 # ineuality
cond5 = "nature" == "water"

cond1 and cond2 # logical AND
cond1 or cond2 # logical OR
not(cond1) # logical NOT

############## Complex ########################################
h = 5 + 10j
type(h)
h.real
h.imag
m = 5 - 10j
h*m

############## Tuple ######################################################
t1 = (5,6,1,4,9) # put values within ()
type(t1)
len(t1)

t2 = 5,6,1,4,9 # this will also create a tuple; not a common practice
t3 = ("nature","water","animals","birds")
t4 = (5,8.9,"nature",True,5+10j) # mix of data types are possible in a tuple

### TUPLE SLICING
t4[0] # indexing starts with 0 and ends with n-1
t4[3]
t4[len(t4)-1] # last
t4[-1]  # last value using negative indexing
t4[-2]
t4[1:4] # range of positions from 1 till 3; excludes the end position given
t4[:4] # beginning till 3
t4[2:] #2nd to last position

## Tuples are immutable. cannot be edited
#t4[1] = 100 throws error

##################### List ####################################################
l1 = [5,6,1,4,9] # created by giving within []
type(l1)
len(l1)
l3 = list(t3) # tuple can be converted to a list
l4 = [1,5.6,"Water",True] # supports mix of data types
t10 = tuple(l1) # convert list to a tuple

## LIST SLICING
l1[0]
l1[-1]
l1[2:4]

### LISTS ARE MUTABLE
l1[1] = 100 # replace value in a position
l1.append(20) # append a value to a list
l1.extend(l4) # extend a list by appending values in another list

l1.index(100) # returns index/position corresponding to a value
l1.index("Water")
l1.index(1) # only index corresponding to 1st occurence will be returned

del l1[3] # deleting based on position
l1.remove("Water") # deleting based on value
l1.remove(1) # only the 1st occurence is removed

l5 = [5,9,3,10,2,6]
l5.reverse() # reverse the order (mirror image)
l5.sort() # sort in ascending
l5.sort(reverse = True) # sort in descending
l3.sort() # even a list of string can be sorted in alphabetical order
#l1.sort() # throws error as mix of data types cannot be sorted

### LIST SEARCHING
# Search for value in a list and return True if it exists
"Water" in l4
5.6 in l4
10 in l4
10 not in l4

### LIST OF LIST
ll1 = [l3,l4]
ll1[0][2]

### LIST OF TUPLE
ll2 = [t1,t3]

#### NUMBER GENERATORS
l10 = list(range(100)) # 100 numbers from 0 till 99
l11 = list(range(1,101)) # 1 till 100
l12 = list(range(1,101,2)) # odd numbers from 1 till 100

rep1 = [5]*50
rep2 = [1,2,3]*10

# Q: Can I create repetition of tuples? Yes
rep3 = (1,2,3)*10

## LIST ASSIGNMENT 
#Create following lists
#a) [1,2,3,….,19,20]
a = list(range(1,21,1))

#b) [20,19,…,2,1]
b = list(range(20,0,-1))

#c) [1,2,3,….19,20,19,18,….,2,1]
c = list(range(1,21)) + list(range(19,0,-1)) # option 1
c = a + b[1:] # option 2
c = a[:-1] + b # option 3

#d) [4,6,3] and assign it to variable tmp
tmp = [4,6,3]

#e) [4,6,3,4,6,3,…..,4,6,3] where there are 10 occurences of [4,6,3]
e = tmp*10

#f) [4,6,3,4,6,3,….,4,6,3,4] where there 10 occurences of [4,6,3] followed by 4
# Option 1
f = tmp*10
f.append(4)

# Option 2
f = e + [4]

#g) [4,4,….,4,6,6,….,6,3,3,….,3] where there are 10 occurences of 4, 20 occurences of 6 and 30 occurences of 3
g = [4]*10 + [6]*20 + [3]*30

#Slice the following from list “f”
# 0th element
f[0]
# last but 3 till last element
f[-4:]
# downsample by 2 (skip alternative samples)
f[::2]

## LIST ADDITIONAL REFERENCE MATERIALS

##################### DICTIONARY ############################################
#{Key: Value}
math_score_list = [95,67,88,45,84]
math_score_list[2] # list supports only positional slicing

math_score_dict = {"Darbin": 95, 
                   "Nagalingam": 67, 
                   "Priya": 88, 
                   "Selva": 45, 
                   "Karthik": 84}
math_score_dict["Karthik"] # extract value using key
math_score_dict["Nagalingam"]
#math_score_dict[2] cannot slice using position

## KEYS HAVE TO BE UNIQUE; Duplicate will override the original
math_score_dict2 = {"Darbin": 95, 
                   "Nagalingam": 67, 
                   "Priya": 88, 
                   "Selva": 45, 
                   "Darbin": 90,
                   "Karthik": 84}

## INSERTING NEW KEY VALUE PAIR
math_score_dict["Bala"] = 79

## DICTIONARY PROPERTIES
math_score_dict.keys()
math_score_dict.values()

####### Values can be of any data type
some_dict1 = {"A": (1,2,3),
              "B": [[1,2,3],[4,5,6]],
              "C": {"AA":5,"BB":9}}

###### Keys should be of immutable data type
some_dict2 = {"A": 5, # string as a key
              4: "hello", # integer as a key
              (6,10): 95} # tuple as a key

#some_dict2 = {"A": 5, # string as a key
#              4: "hello", # integer as a key
#              [6,10]: 95} # list as a key will NOT work

##### Assignment
states = {
    'Oregon':'OR',
    'Florida':'FL',
    'California':'CA',
    'New York': 'NY',
    'Michigan':'MI'}

cities = {
    'CA':'San Fransico',
    'MI': 'Detroit',
    'NY':'Manhattan'}
# what is the name of the city in state "MI"
cities["MI"]

# Add a city 'Orlando' to 'FL'
cities["FL"] = "Orlando"

# what is the city name in 'New York' State
st1=states['New York']
cities[st1]
cities[states['New York']] # above 2 lines in 1 line

## DICTIONARY ADDITIONAL REFERENCE MATERIALS

################### Set ####################################################
# Typically used for set operations
# Values within {}

# set will contain only unique values
ss10 = {5,9,1,4,3,2,7,0,1,2,5,7,3,5,2}
print(ss10)
type(ss10)

seta = {1,2,3,4,5}
setb = {4,5,6,7,8,9,10}

seta & setb # intersection
seta | setb # union (pipe)
seta - setb # exclusive values in set a

#################### date time ###############################################
import datetime # importing functions in datetime package

### DATE
dd = datetime.date(2019,1,6)
type(dd)
print(dd)
dd.day
dd.month
dd.weekday() # Monday starts with a 0
dtod = datetime.date.today() # today's date in system
print(dtod)
dd.weekday()

### DATETIME
dtnow = datetime.datetime.now() # current system clock date and time
dtnow.year
dtnow.minute
dtnow.month

#https://docs.python.org/3/library/datetime.html

### STRPTIME (Custom format to a standard format)
dt1 = datetime.datetime.strptime("28/4/2019","%d/%m/%Y")
dt1.weekday()
datetime.datetime.strptime("28-4-2019","%d-%m-%Y")
datetime.datetime.strptime("28 April, 2019","%d %B, %Y")
datetime.datetime.strptime("28 Apr, 2019","%d %b, %Y")
datetime.datetime.strptime("28th April of 2019","%dth %B of %Y")

### STRFTIME (Standard format to custom format)
datetime.datetime.strftime(dd, "%d/%m/%Y")
datetime.datetime.strftime(dd, "%dth %B of %Y")
datetime.datetime.strftime(dd, "Week %U of Year %Y")

################### numpy #####################################################
### numpy comes pre-installed in anaconda installation
### anyone who uses only Python, then numpy has to be installed 

math_score_list = [95,67,88,45,84]
# What is the average score?
sum(math_score_list)/len(math_score_list)

# What is the median score?
math_score_list.sort()
math_score_list[int(len(math_score_list)/2)] # since the length is odd
# if the length is even, you have to take mid point of 2 centers

######### NUMPY INBUILT MATHEMATICAL FUNCTIONS
import numpy as np # importing numpy with an alias name np
np.mean(math_score_list)
np.median(math_score_list)
np.std(math_score_list)

np.mode(math_score_list) #module 'numpy' has no attribute 'mode'
import statistics
l1 = [10,20,30,10,30,40,50,20,60,10,10,10]
statistics.mode(l1)

from statistics import mean, median, mode, stdev
mean(l1)
median(l1)
mode(l1)
stdev(l1)

########## NUMPY INBUILT NUMBER GENERATORS
ar1 = np.arange(1,101,2) # sequence of numbers
ar2 = np.random.rand(25) # random floating numbers
ar3 = np.random.randint(1,100,50) #  generates 50 integers between 1 and 100
ar4 = np.random.randint(1,100,50) # will be different than ar2 as this is random

## Some times, it might be needed to fix the randomness for resproducing the results
np.random.seed(1234) # fixing the random seed to something
np.random.rand(10)

np.random.seed(435) # fixing the random seed to something else
np.random.rand(10)

############## numpy array ###################################################
type(ar1)
type(ar2)
## all values in a numpy array should be of same data type
## if mix of data types is given, they will automatically get typecasted to 1 common data type

ar10 = np.array([1,False,5.6])
ar11 = np.array([1,False,5.6,"Hello"])

# Q: Are numpy arrays mutable? Yes
# Q: Can a float array be muted with a string value? No, memories are fixed

#### NUMPY ARRAY SLICING

## POSITIONAL SLICING
ar1[0]
ar1[6]
ar1[3:11] # position 3 till position 10
ar1[-5:] # last 5 positions

############ VECTORIZED OPERATION ######################################
## CONDITIONAL (BOOLEAN) SLICING
math_score_list = [95,67,88,45,84]
# extract only values greater than 70
95 > 70
67 > 70
#math_score_list > 70 throws error as a list cannot be compared in 1 go; needs a for loop

math_score_array = np.array(math_score_list) # converting list to an array
cond = math_score_array > 70 # do the condition check on all elements and return a boolean array
math_score_array[cond] # values corresponding to True positions are returned
math_score_array[math_score_array > 70] # above 2 lines in 1 line

math_score_array[math_score_array < 80] # values less than 80

## extract only odd numbers
math_score_array / 2 # dividing each element in array by 2 
math_score_array % 2 # dividing each element in array by 2 and gives the remainder
cond = math_score_array % 2 == 1
math_score_array[cond]
math_score_array[math_score_array % 2 == 1] # in 1 line
## Element wise process
math_score_array2 = math_score_array + 2 # element wise addition
adj_score = np.array([0,10,2,5,3])
math_score_array3 = math_score_array + adj_score # element wise addition of 2 arrays
# above line works because both arrays are of same length
np.array([1,2,3]) + np.array([10,11,12]) # works
#np.array([1,2,3]) + np.array([10,11,12,13]) # doesn't work as the lengths are different


########### ASSIGNMENT 1
math_score_array = np.array([95,67,88,45,84])
eng_score_array = np.array([78,67,45,39,67])
gender_array = np.array(["M","F","F","M","M"])

# extract maths score above average maths score
avg_maths_score = np.mean(math_score_array)
cond = math_score_array > avg_maths_score
math_score_array[cond]

# extract maths score of male students
cond = gender_array == "M"
math_score_array[cond]

# average maths score of male students
np.mean(math_score_array[cond])

# Extract maths score of male students who have scored above 70
# Option 1: Nested slicing
maths_male = math_score_array[gender_array == "M"]
maths_male[maths_male > 70]

# Option 2: Using and operator
cond1 = gender_array == "M"
cond2 = math_score_array > 70
math_score_array[cond1 & cond2]

# Extract maths score of students who are above average in english?
avg_eng_score = np.mean(eng_score_array)
cond = eng_score_array > avg_eng_score
math_score_array[cond]

# Average english score of male students who are above average in maths
cond1 = gender_array == "M"
cond2 = math_score_array > avg_maths_score
np.mean(eng_score_array[cond1 & cond2])

b1 = [True, False, True]
b2 = [True, True, False]
True and False
True or False
#b1 and b2 incorrect answer
#b1 & b2 # list do not support bit wise and
ba1 = np.array(b1)
ba2 = np.array(b2)
ba1 & ba2 #bit wise AND
ba1 | ba2 # bit wise OR

############# ASSIGNMENT 2 (OVER EMAIL)
#Create two numpy arrays with following values
xVec = np.array([42,85,84,23,11,55,14,96,13,30])
yVec = np.array([13,8,85,71, 1,7,55, 2,34,24])

#a. Slice xVec with values greater than 60
cond = xVec > 60
xVec[cond]

#b. Slice yVec with values less than mean of yVec
yvec_mean = np.mean(yVec)
cond = yVec < yvec_mean
yVec[cond]

#c. How many odd numbers in xVec?
xVec % 2 # element wise division by 2 and return the remainder
cond = xVec % 2 == 1
len(xVec[cond]) # option 1
sum(cond) # option 2: sum of a boolean array will return the count of Trues

#d. Slice values in yVec which are between minimum 
 # and maximum values of xVec (yes, xVec)
min(xVec)
max(xVec)
cond1 = yVec > min(xVec)
cond2 = yVec < max(xVec)
yVec[cond1 & cond2]

######################### numpy matrix #######################################
# There is no separate matrix data type
# numpy array can handle any dimensions

# matrix is a 2 dimensional array
## CREATING MATRIX BY RESHAPING A 1D ARRAY
xVec = np.array([42,85,84,23,11,55,14,96,13,30])
type(xVec) # numpy n dimensional array

mat_xv = np.reshape(xVec,[5,2])
mat2_xv = np.reshape(xVec,[2,5])
mat2_xvT = mat2_xv.T # Transpose

#mat3_xv = np.reshape(xVec,[3,4]) throws error as the lengths are mismatching

## CREATING MATRIX BY STACKING ARRAYS
score_mat1 = np.column_stack([math_score_array,eng_score_array]) # column wise stacking
score_mat2 = np.row_stack([math_score_array,eng_score_array]) # row wise stacking

## MATRIX SLICING
#m[row-pos,col_pos]
score_mat1[0,0]
score_mat1[2,1]
score_mat1[:,1] # all rows column 1
score_mat1[2,:] # row 2, all columns
score_mat1[1:4,:] # rows 1 till 3, all columns
score_mat1[[1,3,4],:] # rows 1,3,4; all columns

######################## pandas ###############################################
import pandas as pd

################## pandas Series #############################################
# 1 dimensional data type similar to numpy array
# Series consists of index which allows to slice using unique identifier
xarr = np.array([42,85,84,23])
type(xarr)
xser = pd.Series([42,85,84,23])
type(xser)

xser = pd.Series([42,85,84,23], index = ["a","b","c","d"])
xser[0] # slicing by position
xser["a"] # slicing by index

# Q: Can index be duplicated? Yes
xser2 = pd.Series([42,85,84,23], index = ["a","b","c","b"])
xser2["b"]

xser[0:2]

xser[xser > 50] # vectorized operation
xser.mean() # inbuilt pandas function
xser.median() # inbuilt pandas function
np.mean(xser) # numpy function on pandas series is possible

xser10 = xser/2 # element wise division

# Converting dictionary to a series
# Keys becomes index
states = {
    'Oregon':'OR',
    'Florida':'FL',
    'California':'CA',
    'New York': 'NY',
    'Michigan':'MI'}
states_ser = pd.Series(states)

################# Pandas Dataframe ########################################
# Most popular data type used in the Data science world
# Similar to a SQL table or an Excel table
# Each column can be of different data type
   # In case of matrix, all values should be of same data type
# Typically tabular data imported from files or databases are read as dataframe
   
### CREATING DATAFRAME FROM DICTIONARY OF LIST
dict1 = {"A": [1,2,3],
         "B": [3,4,5]}
df_dict1 = pd.DataFrame(dict1) # Each Key-Value pair becomes a column

dict11 = {"A": [1,2,3],
         "B": [3,4,5,6]}
#df_dict11 = pd.DataFrame(dict11) throws error 
# length of all lists should be equal

## CREATING DATAFRAME FROM DICTIONARY OF SERIES
# Matching happens through index. Mismatching ones get a null
dict2 = {"A": pd.Series([1,2,3], index = ["g","h","i"]),
         "B": pd.Series([3,4,5], index = ["g","h","i"])}
df_dict2 = pd.DataFrame(dict2)

dict22 = {"A": pd.Series([1,2,3], index = ["g","h","i"]),
         "B": pd.Series([3,4,5,6], index = ["g","h","i","j"])}
df_dict22 = pd.DataFrame(dict22)

dict23 = {"A": pd.Series([1,2,3], index = ["g","h","i"]),
         "B": pd.Series([3,4,5], index = ["h","j","i"])}
df_dict23 = pd.DataFrame(dict23)

"""
create a data frame df_emp_details with data of 3 employees
with unique names "Ram","Raj","Ravi" as index.
Create 2 columns Age and Income and assign any integer
"""
## From Dictionary of Series
emp_details_dict1 = {"Age": pd.Series([32,45,25], index = ["Ram","Raj","Ravi"]),
                     "Income": pd.Series([1000,4500,2500], index = ["Ram","Raj","Ravi"])}
df_emp_details = pd.DataFrame(emp_details_dict1)

## From Dictionary of List
emp_details_dict2 = {"Age": [32,45,25],
                     "Income": [1000,4500,2500]}
# adding index in 1 pass together
df_emp_details = pd.DataFrame(emp_details_dict2,
                              index = ["Ram","Raj","Ravi"])

## From numpy matrix
age = [32,45,25]
income = [1000,4500,2500]
emp_mat = np.column_stack([age,income])
df_emp_details = pd.DataFrame(emp_mat,
                              index = ["Ram","Raj","Ravi"],
                              columns = ["Age","Income"])

# Q: How to create dummy matrix with just index and columns
df_emp_empty = pd.DataFrame(index = ["Ram","Raj","Ravi"],
                              columns = ["Age","Income"])

# Q: Can I create dataframe from list of list? Yes
df_emp_details = pd.DataFrame([[32,1000],[45,4500],[25,2500]],
                              index = ["Ram","Raj","Ravi"],
                              columns = ["Age","Income"])

# Single column dataframe is not common. better saved as a Series or array
age_df = pd.DataFrame([32,45,25])

### DATAFRAME PROPERTIES
df_emp_details.shape # returns the number of rows and columns as a tuple
df_emp_details.shape[0] # number of rows
df_emp_details.shape[1] # number of columns
df_emp_details.columns # column names
df_emp_details.columns = ["Age1","Income1"] # Renaming all columns
#df_emp_details.columns = ["Age2"] throws error. it doesn't work this way to rename specific column
# Columns can be selectively renamed by feeding as a key-value pair
df_emp_details = df_emp_details.rename(columns = {"Age1":"Age2"})
df_emp_details.index # row index
df_emp_details.dtypes # data type of each column

## DATAFRAME SLICING

## SLICING COLUMN(S)
df_emp_details["Age2"]
ag = df_emp_details["Age2"] # when 1 column is extracted, it is saved as a series
type(ag) # each column in a dataframe is a series

df_emp_details[["Age2","Income1"]] # Slicing List of columns

#df_emp_details["Ram"] throws error as there is no column "Ram"

## LOC (Slicing by Index)
# df.loc[row_index,col_index]
df_emp_details.loc[:,"Age2"] # similar to df["Age2"]
df_emp_details.loc["Ram",:]
df_emp_details.loc["Ram","Income1"]
df_emp_details.loc[["Ram","Ravi"],"Income1"]
df_emp_details.loc[["Ram","Ravi"],:]

## ILOC (Slicing by Position)
# df.iloc[row_pos,col_pos]
df_emp_details.iloc[:,0]
df_emp_details.iloc[0,:]
df_emp_details.iloc[[0,2],:]
df_emp_details.iloc[1:3,:] # range of positions

## CONDITIONAL SLICING (.LOC)
# SQL: Select * from df_emp_details where Age2 > 30
cond = df_emp_details["Age2"] > 30
df_age_abv30 = df_emp_details[cond]
df_age_abv30 = df_emp_details.loc[cond,:]
# SQL: Select Income1 from df_emp_details where Age2 > 30
income_for_age_abv30 = df_emp_details.loc[cond,"Income1"]

df_age_below30 = df_emp_details.loc[~cond,:] # vectorized not operation using ~

## DATAFRAME SORTING
df_income_sorted = df_emp_details.sort_values("Income1")
df_income_sorted = df_emp_details.sort_values("Income1",
                                              ascending = False)

###########  Practice 
math_score_array = np.array([95,67,88,45,84])
eng_score_array = np.array([78,67,45,39,67])
gender_array = np.array(["M","M","F","M","F"])

# Create a data frame (score_df) with above 3 arrays as columns
# Add "R1001","R1002",...."R1005" as row indexes
# Add "Maths","English","Gender" as column indexes

score_dict = {"Maths": math_score_array,
              "English": eng_score_array,
              "Gender": gender_array}
score_df = pd.DataFrame(score_dict,
                        index = ["R1001","R1002","R1003","R1004","R1005"])
score_df = score_df[["Maths","English","Gender"]]
score_df.dtypes # String columns are saved as object

score_df_maths_sorted = score_df.sort_values("Maths",
                                             ascending = False)
# Note that index also moved along with data while sorting

## ASSIGNMENT 1
## DATAFRAME SLICING

# Slice the following
# Maths column
m = score_df["Maths"]
type(m) # series
# Maths and English Column
list_of_cols_needed = ["Maths","English"]
score_df[list_of_cols_needed] # dataframe
# "Maths" column of "R1001"
score_df.loc["R1001","Maths"]
# "Maths" and English column values of "R1001" and "R1003"
score_df.loc[["R1001","R1003"],["Maths","English"]]
# All rows, 2nd column
score_df.iloc[:,2]
# 0th and 3rd row, 0th and 1st column
score_df.iloc[[0,3],[0,1]]

# data frame of Male students alone
# Select * from score_df where Gender = M
cond = score_df["Gender"] == "M"
score_df_male = score_df[cond]
score_df_male = score_df.loc[cond,:] # same as previous line

# english and maths score of Male students
# Select English, Maths from score_df where Gender = M
score_df.loc[cond,["English","Maths"]]

# all columns of students who score above 70 in Maths
score_df[score_df["Maths"] > 70]

# average maths core of students who got above 60 in English
cond = score_df["English"] > 60
score_df.loc[cond,"Maths"].mean()

# average english score of students who are above average in maths
avg_maths = score_df["Maths"].mean()
cond = score_df["Maths"] > avg_maths
score_df.loc[cond,"English"].mean()

# all columns of male students who scored above 60 in maths
cond1 = score_df["Gender"] == "M"
cond2 = score_df["Maths"] > 60
score_df[cond1 & cond2]

"""
slice english and gender column of either 
female students or students with maths score above 60
"""
cond1 = score_df["Gender"] == "F"
cond2 = score_df["Maths"] > 60
score_df.loc[cond1 | cond2,["English","Gender"]]

## ASSIGNMENT 2 (sent over email)
"""
Generate random integer for 1st column between values 
10 and 100. Note that the number could be different 
in your machine as it is random. 
You can understand the pattern in other columns and row index. 
Make sure you put the column names A, B, C and D
"""
## CREATING FROM MATRIX
mat10 = np.column_stack(
        [np.random.randint(10,100,25),
        [5]*25,
        range(25,0,-1),
        range(0,50,2)])
df = pd.DataFrame(mat10,
                  columns = ["A","B","C","D"],
                  index = range(1001,1026))

## CREATING FROM DICTIONARY
df = pd.DataFrame({
        "A": np.random.randint(10,100,25),
        "B": [5]*25,
        "C": range(25,0,-1),
        "D": range(0,50,2)},
        index = range(1001,1026))

#1. Slice column ‘A’ from df and save it as a series ‘s’
s = df["A"]
type(s) # series

#2. Slice column ‘A’ and column ‘C’ and save it as df2
df2 = df[["A","C"]]

#3. Slice 0th and 2nd column using column number and save it as df3
df3 = df.iloc[:,[0,2]]

#4. Slice from 0 till 5th position in series ‘s’
s[0:5]
s[:5] # same as above

#5. Slice all columns from rows 3 till 19 and save it as df4
df4 = df.iloc[3:20,:]

#6. Create df5 which has subset of data from df where 
 # column A values are above median of column A. 
 # Note: slice entire columns based on condition on column A

# Select * from df where A > median(A)
medA = df["A"].median()
cond = df["A"] > medA
df5 = df[cond]
df5 = df[df["A"] > df["A"].median()] # all in 1 line

################################## Control Instruction ###########################
import numpy as np
############### If ##########################################################

"""
C style
if (condition){
  execute instructions if the condition satisfies
}
else if (condition){

}
else {
}
"""

"""
Python style/ It is based on indendation

if (condition):
    excute instructions
"""

if (7 > 8):
    print("Condition Satisfied")
if (8 > 7): # this is a new condition and has no link with previous condition as it is indended back
    print("Condition Satisfied")

########################3 Nested if #################################
if (7 > 8):
    print("Condition Satisfied")
    if (8 > 7):
        print("Condition Satisfied")

#################### If else #########################################
if (7 > 8):
    print("Condition satisfied")
else: # else will be at the same level as if
    print("Condition not satisfied")

#################### If else if (elif) #############################
x = 0
x = int(input("Enter a number: "))
if (x > 0):
    print("Positive")
elif (x < 0):
    print("Negative")
else:
    print("Neither positive nor negative")

## Q: Can I have an if-else condition within elif? Yes
if (x > 0):
    print("Positive")
elif (x < 0):
    if (x < -100):
        print("High Negative")
    else:
        print("Negative")
else:
    print("Neither positive nor negative")
    
##################### for loop ##########################################
# Used for doing repetitive operations
"""
C Style
for (int i = 0; i < 10; i++){
 # some reptitive operation using iterating variable i
}
"""
# There is no initialization, termination, increment concept in Python
# In Python, looping happens through a set of values
"""
Python style
for i in value
    # some reptitive operation using iterating variable i
"""
for i in [3,7,8,2,3]: # during every iteration, i takes one of the values
    print(i**2)

for i in range(0,10): # for (i = 0; i < 10, i++)
    print(i**2)
   
l1 = [5,6,7,1,2]    
#l2 = [25,36,49,1,4] expected output

l2 = []
for i in l1: # slicing through values in a list
    l2.append(i**2)
print(l2)

l2 = []
for i in range(len(l1)): # slicing through positions
    l2.append(l1[i]**2)
print(l2)

# Wherever possible, avoid appending in loop
   # It is a dynamic memory allocation operation which is computationally expensive
l2 = [0]*len(l1) # pre-allocating memory
for i in range(len(l1)):
    l2[i] = l1[i]**2 # assigning in loop
print(l2)

################### enumerate
## looping through position as well as value

for i in l1: # loops through the values in l1
    print(i) 
    
for i in enumerate(l1): # loops through position and value in l1
    print(i) # tuple 0th index having position and 1st index having the value

l2 = [0]*len(l1)
for i in enumerate(l1):
    pos = i[0]
    val = i[1]
    l2[pos] = val**2
print(l2)

############### LIST COMPREHENSION 
## compact for loops
l2 = [i**2 for i in l1]
print(l2)

## numpy vectorized
a1 = np.array(l1)
type(l1)
type(a1)
#l2 = l1**2 throws error. not possible in list
a2 = a1**2

###### Try this
#l3 = [7,8,9,3,4] # expected output add 2 element wise
# for loop with place holder list created
l3 = [0]*len(l1)
for i in range(len(l1)):
    l3[i] = l1[i] + 2
print(l3)

# list comprehension
l3 = [i + 2 for i in l1]

## Vectorized
#l3 = l1 + 2 throws error not possible in list
a3 = a1 + 2

##### Extract odd numbers from l1
l1 = [5,6,7,1,2]    
#l1_odd = [5,7,1] # expected answer
# for loop
l1_odd = []
for i in l1:
    if (i % 2 == 1):
        l1_odd.append(i) # dynamic appending cannot be avoided as length of final output is not known
print(l1_odd)
# list comprehension
l1_odd = [i for i in l1 if (i % 2 == 1)]
# numpy vectorized
a1_odd = a1[a1 % 2 == 1]

#### Assignment 1 
math_score_list = [95,67,88,45,84]
# Extract math scores greater than 70
#math_abv_70 = [95,88,84] # expected result
# for loop
# list comprehension
# vectorized operation

#### Assignment 2 (age calculator - sent through email)
"""
1. Create a list of birth years of 5 friends/family member 
  (e.g: br_yr = [1986, 1989, 1975, 1981, 1978]). 
  Calculate their age (years alone) as of Dec 31, 2017 
  using 3 approaches and save it a list.
"""
br_yr = [1986, 1989, 1975, 1981, 1978]

# Regular for loops
age = [0]*len(br_yr)
for i in range(len(br_yr)):
    age[i] = 2017 - br_yr[i]
print(age)

# List comprehension
age = [(2017 - i) for i in br_yr]

# Vectorized operation using numpy array
br_yr_np = np.array(br_yr)
age = 2017 - br_yr_np

#### Assignment 3 (stop word removal - sent through email)
"""
2. Create a string “this is a python exercise which is neither 
 too easy nor too hard to be solved in the given amount of time”. 
 Split the string to list of individual words [Hint: split command. 
 Don’t search in classwork]. 
 Remove words like ‘is’, ‘a’ and ‘the’ programmatically using 3 approaches
"""
stmt = "this is a python exercise which is neither too easy nor too hard to be solved in the given amount of time"
stmt_words = stmt.split(" ")
# Regular for loops
stop_words = ["is","a","the"]
stmt_words_stopword_removed = []
for i in stmt_words:
    if (i not in stop_words):
        stmt_words_stopword_removed.append(i)
print(stmt_words_stopword_removed)
        
# List comprehension
stmt_words_stopword_removed = [i for i in stmt_words if i not in stop_words]

# Vectorized operation using numpy array
stmt_words_np = np.array(stmt_words)

## Option 1
cond1 = stmt_words_np != "is"
cond2 = stmt_words_np != "the"
cond3 = stmt_words_np != "a"
stmt_words_np_stopwords_removed = stmt_words_np[cond1 & cond2 & cond3]

## Option 2
# Vectorized in operation of numpy
a10 = np.array([1,2,3,4,5])
a11 = np.array([1,4,5])
a10[np.isin(a10,a11)] # values in a10 which are present in a11
a10[np.in1d(a10,a11)] # for older versions of numpy

a10[~np.isin(a10,a11)] # values in a10 which are NOT present in a11
a10[~np.in1d(a10,a11)] # for older versions of numpy

stmt_words_np_stopwords_removed = \
    stmt_words_np[~np.isin(stmt_words_np,stop_words)]

# Option 3
stmt_words_np_stopwords_removed = \
        np.setdiff1d(stmt_words_np,stop_words)

# Q: What is the difference between looping through list vs numpy array
l1 = [1,2,3,4,5]
a1 = np.array(l1)

for i in l1:
    print(i**2)

for i in a1: # you can loop through array
    print(i**2)
a1**2 # but many operations can be done without using for loop; vectorized operation



######### Nested for loop
ll10 = [[1,2,3],[5,6,7]]

for i in ll10:
    ll11 = i
    for j in ll11:
        print(j**2)
        
    

################### While loop ########################################
# for loop will have a start and stop
# while loop stops conditionally. there is no default stop

x = 2    
while (x < 20): # loop runs till this condition fails
    print(x**2)
    x = x + 2

""" Infinite loop
while loops can tend to loop infinitely      
x = 2    
while (x < 20): # loop runs till this condition fails
    print(x**2)
    x = x - 2
"""

################# break ##########################################
## In the machine learning and Operation research world, 
  # optimization algorithms are written within while loops
  # generally the iterations will break after a threshold

x = 2    
max_iter = 1000 # typically set as a parameter for optimization problems
iter_count = 0
while (x < 20): # loop runs till this condition fails
    print(x**2)    
    #### Let's assume below line to be an optimization logic
    x = x - 2
    ####    
    iter_count = iter_count + 1
    if (iter_count > max_iter):
        print("I am done. Going to break") # this will be executed
        break # terminates from the parent loop
        print("Breaking complete") # this will NOT be executed
    
    
#### Detect whether a number is prime
        # it should not be divisible any other number than 1 and itself
## break can be used with for loop as well
# 1113
n = 1117
for i in range(2,n):
    if (n % i == 0):
        print("not a prime. It is divisible by ", i)
        break

################ Continue  ##################################
 # Used for skipping iterations
###### Amazon big summer sale
## Flat 10% discount on all products * conditions apply
 # * discount not applicable for products already on discount

price = [100,89,997,45,67,105]
existing_discount = [0,5,10,0,0,3]

price_big_summer_sale = price.copy()
# for loop
for i in range(len(price)):
    if (existing_discount[i] > 0):
        continue
    #### Some complex logics for 20 lines
    price_big_summer_sale[i] = round(price[i]*0.9)
print(price_big_summer_sale)

# Q: Can I apply only the difference discount?
price_big_summer_sale = price.copy()
# for loop
for i in range(len(price)): 
    difference_disc = 10 - existing_discount[i]
    price_big_summer_sale[i] = round(price[i]*(1-difference_disc/100))
print(price_big_summer_sale)
        
############################# File processing ###
import pandas as pd
import os

acs2008 = pd.read_csv("C:\\Karthik\\Learning\\Python\\Green\\data\\ACS_08_3YR_S1903.csv")
acs2008 = pd.read_csv("C:/Karthik/Learning/Python/Green/data/ACS_08_3YR_S1903.csv")
# Backslash could get confused with escape characters \n, \t \r
# putting an r before string tells python to not treat anything as escape character
acs2008 = pd.read_csv(r"C:\Karthik\Learning\Python\Green\data\ACS_08_3YR_S1903.csv")

acs2013 = pd.read_csv(r"C:\Karthik\Learning\Python\Green\data\ACS_13_5YR_S1903.csv")
## It is not recommended to give absolute full path
## Relative path from working directory is a suggested approach
## F5 in spyder sets the working directory to the folder where script is present
os.getcwd() # gives the current working directory
os.chdir(r"C:\Karthik\Learning\Python\Green") # changes the directory to given path
os.getcwd()

acs2008 = pd.read_csv("data/ACS_08_3YR_S1903.csv")
acs2013 = pd.read_csv("data/ACS_13_5YR_S1903.csv")

acs2013.shape
# Since pandas assigns the data type by itself, it is good to review the same
acs2013.dtypes
# String data types are saved as objects in dataframe
# GEO.id2 is a string (object) but incorrectly read as integer
acs2013 = pd.read_csv("data/ACS_13_5YR_S1903.csv",
                      dtype = {"GEO.id2": str})
acs2013.dtypes # Now GEO.id2 is an object

acs2008 = pd.read_csv("data/ACS_08_3YR_S1903.csv", 
                      dtype = {"GEO.id2": str})

acs2013_stats_summary = acs2013.describe()

## Try following questions for ACS 2013 data
# Q1. slice the first 7 columns
acs2013_s = acs2013.iloc[:,:7]
# Q2. rename the column names as follows
acs2013_s.columns = ["ID","FIPS","State",
                    "Total Household", "Total Household MOE",
                    "Income","Income MOE"]
# Q3. calculate average income of US
acs2013_s["Income"].mean()

############ Assignment. Answer the following
# Q4. what is the maximum income and which state is that?
# Q5. what is the minimum income and which state is that?
# Q6. get the list of states which are above average in household income
# Q7. get the income of texas state
# Q8. what is the state which has the 2nd highest income

############# ASSIGNMENT AIRQUALITY

############ MERGING FILES ################################################
# Similar to JOIN operations in SQL and LOOKUP in Excel

# Employee personal details (Emp ID, Name, Phone, EMail ID, #no of dependents, address)
# Employee org details (Emp ID, Dept ID, Dept Name, Designation, Reporting manager EMp ID)
# Employee comp details (Emp ID, Basic, HRA, PF)


# Transaction table (Product ID, Store ID, Unit sales, Price per unit, Coupon)
# Product master (Product ID, Product name, Manufacturer name, Brand, Pack size)
# Store master (Store ID, Store address, Lat, Long)

# While merging there could be mismatches in primary key between 2 tables

# Creating a similar scenario in ACS data by removing some keys (FIPS) from each table
# Alaska (FIPS 02) data is removed from ACS 2013
acs2013_rowremoved = pd.read_csv("data/ACS_13_5YR_S1903_rowremoved.csv",
                      dtype = {"GEO.id2": str})
acs2013_rowremoved_s = acs2013_rowremoved.iloc[:,[1,3,5]]
acs2013_rowremoved_s.columns = ["FIPS","HOUSEHOLDS","INCOME"]

# Albama (FIPS 04) data is removed from ACS 2008
acs2008_rowremoved = pd.read_csv("data/ACS_08_3YR_S1903_rowremoved.csv",
                      dtype = {"GEO.id2": str})
acs2008_rowremoved_s = acs2008_rowremoved.iloc[:,[1,3,5]]
acs2008_rowremoved_s.columns = ["FIPS","HOUSEHOLDS","INCOME"]

# Q: Can I programatically remove a row from a dataframe?
acs2013_pgmticaly_rowdropped = acs2013_s.drop(4, axis = 0) # removing a row
acs2013_pgmticaly_coldropped = acs2013_s.drop("FIPS", axis = 1) # removing acolumn

# Q: Can I drop using position? Extract index rom position and then feed to drop
indx_of_a_pos = acs2013_s.columns[1]
acs2013_pgmticaly_coldropped = acs2013_s.drop(indx_of_a_pos, axis = 1)

# FIPS 04 is NOT present in ACS 2008 but present in ACS 2013
# FIPS 02 is NOT present in ACS 2013 but present in ACS 2008
# There are mismatches between the 2 tables being merged

## OUTER JOIN
# All records will be present. Nulls wherever there is mismatch

merged_outer = pd.merge(acs2008_rowremoved_s, acs2013_rowremoved_s,
                        how = "outer", on = "FIPS")
# if the other column names are same, left table suffixed with x and right with y
 # the suffix string can be customized
merged_outer = pd.merge(acs2008_rowremoved_s, acs2013_rowremoved_s,
                        how = "outer", on = "FIPS",
                        suffixes = ("_2008","_2013"))

## If the column names of primary key are different, left_on and right_on to be given
acs2013_rowremoved_s.columns = ["FIPS_2013","HOUSEHOLDS_2013","INCOME_2013"]
acs2008_rowremoved_s.columns = ["FIPS_2008","HOUSEHOLDS_2008","INCOME_2008"]
merged_outer = pd.merge(acs2008_rowremoved_s, acs2013_rowremoved_s,
                        how = "outer", 
                        left_on = "FIPS_2008", right_on = "FIPS_2013")

merged_outer = merged_outer.sort_values("FIPS_2013")

## INNER JOIN
# Only overlapping records will be present
merged_inner = pd.merge(acs2008_rowremoved_s, acs2013_rowremoved_s,
                        how = "inner", 
                        left_on = "FIPS_2008", right_on = "FIPS_2013")

## LEFT JOIN
# Left table becomes the reference
merged_left = pd.merge(acs2008_rowremoved_s, acs2013_rowremoved_s,
                        how = "left", 
                        left_on = "FIPS_2008", right_on = "FIPS_2013")

## RIGHT JOIN
# Right table becomes the reference.
merged_right = pd.merge(acs2008_rowremoved_s, acs2013_rowremoved_s,
                        how = "right", 
                        left_on = "FIPS_2008", right_on = "FIPS_2013")


#################### FILE WRITIG #####################################
merged_inner.to_csv("output/merged_20190519.csv")

# If index should not go as a separate column, set it to False
merged_inner.to_csv("output/merged_20190519.csv", index = False)


# Q: Can I join multiple tables?
# It has to be done in a sequence
#pd.merge(pd.merge(pd.merge(tableA,tableB),tableC),tableD)


############################# Air Quality #######

import pandas as pd
import numpy as np

airquality = pd.read_csv("data/airquality.csv")

#Data Frame properties and quality check
airquality.dtypes

#3. How many rows in the data?
airquality.shape[0]

#4. How many columns in the data?
airquality.shape[1]

#5. What are the column names?
airquality.columns

#6. How many null values in Ozone column (Note: nans are treated as nulls. 
 # There is a pandas function to catch nulls)
airquality["Ozone"].isnull().sum() # sum on boolean array will return count of Trues
len(airquality.loc[pd.isnull(airquality["Ozone"]),"Ozone"]) # alternative option

#
#Data Frame slicing
#7. Slice from airquality a dataframe which only has rows 
 # with valid entries for Solar.R. 
 # Remove rows which has null values in Solar.R column
aq_solarna_removed = airquality[airquality["Solar.R"].notnull()] # option 1
aqclean = airquality.dropna() # any row with missing value will be removed 
aq_solarna_removed = airquality.dropna(subset = ["Solar.R"]) # option 2. checking for missing value in particular column

#8. What is the average value of Ozone column?
airquality["Ozone"].mean()

#9. What is the average value of Solar.R on days with 
 # temperature above average temperature?
cond = airquality["Temp"] > airquality["Temp"].mean()
airquality.loc[cond,"Solar.R"].mean()

#10. Slice only records of 15th day of each month
aq_day15 = airquality[airquality["Day"] == 15]

#11. Slice records of 6th and 8th month alone
aq_68 = airquality[(airquality["Month"] == 6) | 
        (airquality["Month"] == 8)] # option 1
aq_68 = airquality[airquality["Month"].isin([6,8])] # option 2

#12. What is the average ozone values of the days where both Solar.R and 
 # Temperature are above their averages?
cond1 = airquality["Solar.R"] > airquality["Solar.R"].mean()
cond2 = airquality["Temp"] > airquality["Temp"].mean()
airquality.loc[cond1 & cond2, "Ozone"].mean()

#
#For loops
#13. Calculate average values of Ozone, Solar, Wind and Temperature 
 # and save in a list/array/series
aq_avg_values = [airquality["Ozone"].mean(),
            airquality["Solar.R"].mean(),
            airquality["Wind"].mean(),
            airquality["Temp"].mean()]
# above approach is manual
cols_needed = airquality.columns[:4]
aq_avg_values = pd.Series([0.0]*len(cols_needed), index = cols_needed)
for i in cols_needed:
    aq_avg_values[i] = airquality[i].mean()
print(aq_avg_values)

#14. Calculate month-wise average Ozone and save in a list/array/series
airquality.loc[airquality["Month"] == 5,"Ozone"].mean()
airquality.loc[airquality["Month"] == 6,"Ozone"].mean()
airquality.loc[airquality["Month"] == 7,"Ozone"].mean()
airquality.loc[airquality["Month"] == 8,"Ozone"].mean()
airquality.loc[airquality["Month"] == 9,"Ozone"].mean()
# Above approach is too manual

uniq_months = airquality["Month"].unique()
monthwise_avg_ozone = pd.Series([0.0]*len(uniq_months),
                                index = uniq_months)
for i in uniq_months:
    monthwise_avg_ozone[i] = airquality.loc[airquality["Month"] == i,
                        "Ozone"].mean()
print(monthwise_avg_ozone)


#15. Calculate month-wise average Ozone, Solar, Wind and Temperature and 
 # save in a matrix/data frame. [Hint: nested for loop?]

# Creating datframe from matrix
monthwise_avg_allcols = pd.DataFrame(np.zeros([len(uniq_months),
                                               len(cols_needed)]),
                        columns = cols_needed, index = uniq_months)

# Creating dummy dataframe directly
monthwise_avg_allcols = pd.DataFrame(columns = cols_needed, 
                                     index = uniq_months,
                                     dtype = float)
monthwise_avg_allcols.dtypes
for i in uniq_months:
    for j in cols_needed:
#        print("Month = ", i, "Column = ", j)
        monthwise_avg_allcols.loc[i,j] = \
            airquality.loc[airquality["Month"] == i,j].mean()

############################# mtcars Assignment ###########

import pandas as pd
import numpy as np

mtcars = pd.read_csv("data/mtcars.csv")

#Data Frame properties and quality check
#3. How many rows in the data?
mtcars.shape[0]
#4. How many columns in the data?
mtcars.shape[1]
#5. What are the column names?
mtcars.columns
#6. Use describe command to understand the statistical summary.
mtcars.describe()

#Data Frame slicing
#7. Average miles per gallon (mpg) of all cars
mtcars["mpg"].mean()
np.mean(mtcars["mpg"])
#8. Average mpg of automatic transmission cars
cond = mtcars["am"] == 0
mtcars.loc[cond,"mpg"].mean()

#9. Average mpg of manual transmission cars
cond = mtcars["am"] == 1
mtcars.loc[cond,"mpg"].mean()

#10. Average Displacement of cars with 4 gears
cond = mtcars["gear"] == 4 
mtcars.loc[cond,"disp"].mean()

#11. Average Horse power of cars with 3 carb
#12. Average mpg of automatic cars with 4 gears
cond1 = mtcars["am"] == 0
cond2 = mtcars["gear"] == 4
mtcars.loc[cond1 & cond2, "mpg"].mean()

#13. Average qsec of cars with mpg above average mpg and 
  # weight below average weight
cond1 = mtcars["mpg"] > mtcars["mpg"].mean()
cond2 = mtcars["wt"] < mtcars["wt"].mean()
mtcars.loc[cond1 & cond2, "qsec"].mean()

#14. Entire row of the vehicle which has the highest miles per gallon
#15. Entire row of vehicle with the highest horsepower
mtcars.loc[mtcars["hp"].idxmax(),:] # option 1
mtcars.loc[mtcars["hp"] == mtcars["hp"].max(), :] # option 2

#16. Mileage and hp of car with highest weight
mtcars.loc[mtcars["wt"].idxmax(),["mpg","hp"]] # option 1
mtcars.loc[mtcars["wt"] == mtcars["wt"].max(), ["mpg","hp"]] # option 2

#17. Calculate ratio of mpg to carb for each car and calculate the average of ratio
np.mean(mtcars["mpg"]/mtcars["carb"])

#18. Weight of the car with the minimum displacement
#19. Slice all columns of 3 gear cars
#20. Slice mpg, displacement and hp columns of manual transmission cars

#For loops
"""
21. What is
a. average mpg for 3 gear cars
b. average mpg for 4 gear cars
c. average mpg for 5 gear cars
Save result in a list/array/series
"""
mtcars.loc[mtcars["gear"] == 3, "mpg"].mean()
mtcars.loc[mtcars["gear"] == 4, "mpg"].mean()
mtcars.loc[mtcars["gear"] == 5, "mpg"].mean()
# above process is manual

unique_gears = np.unique(mtcars["gear"])
gearwise_avgmpg = pd.Series([0.0]*len(unique_gears),
                            index = unique_gears)
for i in unique_gears:
    gearwise_avgmpg[i] = mtcars.loc[mtcars["gear"] == i, "mpg"].mean()

"""
22. What is
a. average hp, average wt, average sec, average vs for 3 gear cars
b. average hp, average wt, average sec, average vs for 4 gear cars
c. average hp, average wt, average sec, average vs for 5 gear cars
Save list in a matrix/data frame
"""
cols_needed = ["hp","wt","qsec","vs"]
# Creating dummy dataframe directly
gearwise_avg_multicols = pd.DataFrame(columns = cols_needed, 
                                     index = unique_gears,
                                     dtype = float)
for i in unique_gears:
    for j in cols_needed:
        gearwise_avg_multicols.loc[i,j] = \
            mtcars.loc[mtcars["gear"] == i, j].mean()

######### Apply Aggregate
#23. Solve 21 without for loop
# gearwise average mpg
mtcars_groupby_gear = mtcars.groupby("gear")
mtcars_groupby_gear["mpg"].agg(np.mean)

#24. Solve 22 without for loop
mtcars_groupby_gear[cols_needed].agg(np.mean)

#25. average hp, median and average wt, average vs for different gear-transmission combinations
mtcars_groupby_gear.agg({"hp": np.mean,
                         "wt": [np.mean, np.median],
                         "vs": np.mean})

############################# Data visualisation ############

import pandas as pd
 # pandas has inbuilt plots which internally used matplotlib
import numpy as np
import matplotlib.pyplot as plt

wgdata = pd.read_csv("data/wg.csv")

############### EXPLORATORY ANALYSIS AND DATA QUALITY CHECKS #############
#1. number of nans in wg column
wgdata["wg"].isnull().sum()

wgdata["wg"].isna().sum()

#2. number of nans in metmin
wgdata["metmin"].isnull().sum()

#3. how many observations had a null either in wg or in metmin column?
(wgdata["wg"].isnull() | wgdata["metmin"].isnull()).sum()

#4. extract the observations which don't have null in "wg" or "metmin" columns
wg2 = wgdata[wgdata["wg"].notnull() & wgdata["metmin"].notnull()]

wg2 = wgdata.dropna(subset = ["wg","metmin"]) # alternative option
wgclean = wgdata.dropna() # row will be dropped even if one of the columns has missing valies

#5. how many people have gained weight above average weight gain of the data
(wgdata["wg"] > wgdata["wg"].mean()).sum()

#6. what is the gender of the person with the highest weight gain?
wgdata["wg"] == wgdata["wg"].max() # returns True wherever max value is
wgdata["wg"].idxmax() # index where max value is
wgdata.loc[wgdata["wg"].idxmax(),"Gender"]

#6a. In this data, who gained more weight, male or female?
wgdata.groupby("Gender")["wg"].agg(max)

wgdata.groupby("Gender")["wg"].agg(np.mean)
wgdata.groupby("Gender")["wg"].agg(np.median)
# On an average, wg of Male is more than Female

#7. get the count of above avg wg people by shift
wg_abvavg_wg = wgdata.loc[wgdata["wg"] > wgdata["wg"].mean()] 
wg_abvavg_wg.groupby("Shift").size()

#8. percentage of male in each shift
(wgdata["Gender"] == "M").sum()/wgdata.shape[0] # Overall there 28.9% Male
wg_male = wgdata[wgdata["Gender"] == "M"]
male_cnt_byshift = wg_male.groupby("Shift").size() # count of male in each shift
emp_cnt_byshift = wgdata.groupby("Shift").size() # count of people in each shift
male_cnt_byshift*100/emp_cnt_byshift

#9. Create a copy of the data frame and impute missing values with mean of the column
wg2 = wgdata.copy()
wg2["wg"] = wg2["wg"].fillna(wgdata["wg"].mean())
wg2["metmin"] = wg2["metmin"].fillna(wgdata["metmin"].mean())

##################### Histogram #############################################
# It is a type of bar chart which provides view of distribution
# The input array is binned and count of observations in each bin is plotted as bar

income_rand = np.random.randint(10000,600000,2000)
np.mean(income_rand)
np.median(income_rand)
min(income_rand)
max(income_rand)
plt.figure()
plt.hist(income_rand) # income divided into 10 bins which are equidistant
plt.figure()
plt.hist(income_rand, bins = range(10000,600000,20000)) # custom binning

income_norm = np.random.normal(30000,10000,2000)
plt.figure()
plt.hist(income_norm)

## six sigma
## any deviation beyond 3 times standard deviation on either sides is not meeting quality

# Plot the histogram distribution of weight gain (wgclean)
plt.figure()
plt.hist(wgclean["wg"])

plt.figure()
plt.hist(wgclean["wg"], bins = [0,10,20,30,40,50,60,70])

plt.figure()
plt.hist(wgclean["wg"], bins = range(0,80,10),
         color = "red", edgecolor = "black")
plt.xlabel("Weight Gain Bins (lbs)")
plt.ylabel("Number of people")
plt.title("Distribution of Weight Gain")
plt.xlim([0,60])

#################### subplot ###############################################
# splits the figure into sub sections
wgclean_m = wgclean[wgclean["Gender"] == "M"]
plt.hist(wgclean_m["wg"])
wgclean_f = wgclean[wgclean["Gender"] == "F"]
plt.hist(wgclean_f["wg"])

f, (ax1, ax2) = plt.subplots(1, 2)
ax1.hist(wgclean_m["wg"], color = "blue")
ax1.set_title("Male")
ax2.hist(wgclean_f["wg"], color = "red")
ax2.set_title("Female")

# Shared y axis between graphs
f, (ax1, ax2) = plt.subplots(1, 2, sharey = True)
ax1.hist(wgclean_m["wg"], color = "blue")
ax1.set_title("Male")
ax2.hist(wgclean_f["wg"], color = "red")
ax2.set_title("Female")

f, (ax1, ax2) = plt.subplots(2, 1, sharex = True, sharey = True)
ax1.hist(wgclean_m["wg"], color = "blue")
ax1.set_title("Male")
ax2.hist(wgclean_f["wg"], color = "red")
ax2.set_title("Female")


fig, axes = plt.subplots(2, 2)
axes[0, 0].hist(wgclean_m["wg"], color = "blue")
axes[0, 0].set_title("Weight Gain of Male")
axes[0, 1].hist(wgclean_m["metmin"], color = "blue")
axes[0, 1].set_title("Activities of Male")
axes[1, 0].hist(wgclean_f["wg"], color = "red")
axes[1, 0].set_title("Weight Gain of Female")
axes[1, 1].hist(wgclean_f["metmin"], color = "red")
axes[1, 1].set_title("Activities of Female")

## Pandas histogram
wgclean["wg"].plot.hist()

# Pandas supports dataframe plot and will automatically add legend
income_comparison = pd.DataFrame({
        "CompanyA": np.random.normal(30000,5000,1000),
        "CompanyB": np.random.normal(40000,7000,1000)})
income_comparison.plot.hist()


################### Scatter Plot #####################################
#### X - Y Plot
# Compare 2 variables 

## Matplotlib
plt.scatter(wgclean["metmin"],wgclean["wg"], c = "red")
plt.xlabel("Activities")
plt.ylabel("Weight Gain")

## Pandas scatter
wgclean.plot.scatter("metmin","wg", c = "red") # x and y labels are automatically added

############## Bar Plot #################################################
genderwise_avg_wg = wgclean.groupby("Gender")["wg"].apply(np.mean)
genderwise_avg_wg.plot.bar()

shiftwise_avg_wg = wgclean.groupby("Shift")["wg"].apply(np.mean)
shiftwise_avg_wg.plot.bar()

gendershiftwise_avg_wg = wgclean.groupby(["Shift","Gender"])["wg"].apply(np.mean)
gendershiftwise_avg_wg.plot.bar()

############# Pie Plot ##################################################
genderwise_count = wgclean.groupby("Gender").size()
genderwise_count.plot.pie()

shiftwise_count = wgclean.groupby("Shift").size()
shiftwise_count.plot.pie()

############## Line Plot #################################################
stockdata = pd.read_csv("data/Stock_Price.csv")
# Average weekly closing stock prices of Dell and Intel from Jan 2010
plt.plot(stockdata["DELL"]) # matplotlib
plt.plot(stockdata["Intel"]) 
plt.plot(stockdata) # plotting multiple lines in a dataframe

stockdata.plot.line() # pandas; legend is automatically added

# When date-time becomes index, the line plots will become time series plots
pd.date_range(start = "2010-01-01", periods = 76, freq = "W") # Weekly
pd.date_range(start = "2010-01-01", periods = 76, freq = "D") # Daily
pd.date_range(start = "2010-01-01", periods = 76, freq = "M") # Monthly

stockdata.index = pd.date_range(start = "2010-01-01", periods = 76, freq = "W")
stockdata.plot.line() # pandas
# x axis tick labels will automatically adjust based on the periods and frequency

#################### Box Plot ###############################################
### A plot which provides the distribution

a = np.random.rand(100)
np.mean(a)
np.median(a)
# Generally mean and median will be close to each other

a_withoutliers = np.append(a,[10000,98000])
np.mean(a_withoutliers) # mean is affected by outliers
np.median(a_withoutliers) # median is robust to outliers

## Tukey's Boxplot
# Provides the distribution using percentiles than mean and standard deviation 
np.random.seed(1234)
income_norm = np.random.normal(30000,10000,2000)
plt.boxplot(income_norm)

# Percentiles provide relative ordering
# 1st quartile/25th percentile
q1 = np.percentile(income_norm,25);#23634 # 25% of population less than this value
# 2nd quartile/ 50th percentile/ median
q2 = np.median(income_norm); # 30223
q2 = np.percentile(income_norm,50) # median and 50th percentile are same
# median is a mid point where 50% of the population are on either sides
# 3rd quartile/ 75th percentile
q3 = np.percentile(income_norm,75) # 36745; 75% of population less than this value

## Inter Quartile Range
iqr = q3 - q1

## Whisker lines
# Values which are outside these lines can be treated as outliers
# Upper Whisker line
uwl = q3 + 1.5*iqr # 56858
# Lower whisker line
lwl = q1 - 1.5*iqr # 3967

### Boxplot of weight gain
plt.boxplot(wgclean["wg"])
wgclean["wg"].describe()
q1 = 8
q2 = 15
q3 = 20
iqr = q3 - q1  # 12
uwl = q3 + 1.5*iqr # 38
lwl = q1 - 1.5*iqr # -10
# whisker lines are capped at min and max
# Since minimum is 2, lwl is capped at 2
uwl = min(q3 + 1.5*iqr, max(wgclean["wg"])) # 38
lwl = max(q1 - 1.5*iqr, min(wgclean["wg"])) # 2

a = np.append(np.random.rand(100), np.random.rand(100)*-1)
plt.boxplot(a)

### Pandas Box Plot
wgclean["wg"].plot.box()

# Boxplot is popularly used to compare distribution between categorical levels
wgclean.boxplot(column = "wg", by = "Gender")
wgclean.boxplot(column = "wg", by = "Shift")
wgclean.boxplot(column = "wg", by = ["Shift","Gender"])

# Q: How to save the plot as image file?
plt.boxplot(wgclean["wg"])
plt.savefig("output/wgdistribution.png")

################### Assignment ################################
airquality = pd.read_csv("data/airquality.csv")
#Airquality
#1. Get the histogram distribution of Solar.R
plt.hist(airquality["Solar.R"])
plt.hist(airquality["Temp"])
airquality["Solar.R"].plot.hist()
#2. Get the boxplot distribution of temperature
airquality["Temp"].plot.box()
airquality["Temp"].describe()
#3. Generate a scatter plot between temperature and solar.R
airquality.plot.scatter("Temp","Solar.R")
#4. Generate a line plot of Solar.R. 
 # Create a date column and use that as x axis in line plot. 
 # Note: Date and time available in data frame. 
 # You may have to refer the documentation of data to know the year.

airquality["Solar.R"].plot.line()
airquality["Temp"].plot.line()

airquality.index = pd.date_range(
        start = "1973-05-01", periods = 153,
              freq = "D")
# above approach of creating date sequence may not be 
#correct if there are missing dates in the data (e.g: stock market)
airquality.index = pd.to_datetime({
        "day": airquality["Day"],
        "month": airquality["Month"],
        "year": 1973})

airquality["Temp"].plot.line()


#Mtcars
mtcars = pd.read_csv("data/mtcars.csv")
#1. Compare the mpg boxplot distribution of automatic vs manual transmission cars
mtcars.boxplot(column = "mpg", by = "am")
#2. Compare the boxplot distribution of mpg of cars by gears and transmission. 
 # One mpg distribution box per gear-am combination
mtcars.boxplot(column = "mpg", by = ["gear","am"])

#3. Generate a scatter plot between mpg and weight of the car
mtcars.plot.scatter("wt","mpg")
mtcars.plot.scatter("mpg","wt")

##################### Functions ####################

def print_with_excalamtion(word):
    print(word + "!!!!")

def print_sum(x,y):
    print(x + y)
    
def print_sum2(x = 0, y = 0):
    print(x + y)

def print_val(v):
    v = v + 1
    print(v)
	
def calc_sum(x,y):
    s = x + y
    return(s)

def sum_diff2(x,y):
    s = x + y
    d = x - y
    return(s,d)

def calculate(x,y,option):
    if (option == "sum"):
        res = x + y
    elif (option == "diff"):
        res = x - y
    elif (option == "mult"):
        res = x*y
    elif (option == "div"):
        res = x/y
    else:
        print("unknown option. returning zero")
        res = 0
    return(res)

polynom_l = lambda x: x**2 + 5*x + 4

polynom_l2 = lambda x=0,y=0: x**3 + 3*x*y + y**2 + 15

# A set of logic which is used frequently can be written as function
# Modular
# Code maintenance

## Inbuilt Functions
print()
sum()
min()

## Functions from packages
np.mean()
pd.Series()

## User defined functions
"""
def fn_name(input):
    # do some operations
    return(output)
"""
def print_with_exclamation(ip_str):
    print(ip_str + "!!!!!!!!!!")

print_with_exclamation("Hello")
print_with_exclamation("Oops")

def add5(ip_nr):
    op1 = ip_nr + 5
    return(op1)

a = add5(10)
b = add5(25)

def calc_sum(x,y):
    op1 = x + y
    return(op1)

calc_sum(5,10)
#calc_sum(5) throws error

### Function with default input values
def cal_sum2(x = 0,  y = 0):
    op1 = x + y
    return(op1)

cal_sum2(5,10)
cal_sum2(5)
cal_sum2()

def cal_sum3(x, y= 0):
    return(x + y)
cal_sum3(5)
cal_sum3()

"""
Following function throws error as default arguments cannot come first
def cal_sum4(x=0, y):
    return(x + y)
"""
### You can return only one object from a function
## More than one outputs can be wrapped within an object like tuple,list etc.
def calc_sum_diff(x,y):
    op1 = x + y
    op2 = x - y
    return([op1,op2]) # returning a list

res1 = calc_sum_diff(5,10)
s,d = calc_sum_diff(5,10)

def calc_sum_diff2(x,y):
    op1 = x + y
    op2 = x - y
    return(op1,op2) # returning a tuple

res2 = calc_sum_diff2(5,10)
# output in tuple form can be collected as separate variables
s,d = calc_sum_diff2(5,10)



def calculate(x,y,option):
    if (option == "sum"):
        re = x + y
    elif (option == "diff"):
        re = x - y
    elif (option == "mult"):
        re = x * y
    elif (option == "div"):
        re = x / y
    elif (option == "customcalc1"):
        re = (x + y)/2
    else:
        print("unknown input. returning null")
        re = None
    return(re)

### Positonal Matching
op1 = calculate(5,3,"sum") # 8
op2 = calculate(5,3,"diff") # 2
op4 = calculate(5,3,"mult") # 15
op6 = calculate(6,3,"div") #2
op7 = calculate(8,4,"junk")
op8 = calculate(10,5,"customcalc1")
calculate("sum",5,10) 
## Argument Matching
calculate(option = "diff", x = 5, y = 3)
calculate(y = 3, option = "diff", x = 5)


######### Scope of function

def some_fn(v): # scope of variable v is inside the function
    v = v + 5
    return(v)
    
some_fn(10)
#print(v) throws error as v is not accessible outside function

v = 50
some_fn(v)
print(v) # still 50 becuase this "v" is different from "v" in function

############## Lambda function ##########################
# compact 1 line function
# fn_name = lambda ip1, ip2: do something with inputs

def calc1(x):
    y = x**2 + 3*x + 5
    return(y)
op10 = calc1(10)

calc2 = lambda x: x**2 + 3*x + 5
op11 = calc2(10)

# Function with 2 inputs
def calc3(x,z):
    y = 3*x**3 + 5*x*z + z**2 + 10
    return(y)
calc3(5,10)

calc4 = lambda x,z: 3*x**3 + 5*x*z + z**2 + 10
calc4(5,10)

# Function with multiple outputs
calc5 = lambda x: (x**3 + 2*x**2 + 10, 5*x**2 + 10)
op15, op16 = calc5(6)

## A function like below cannot be written in a lambda
def calc10(x):
    a = x**2
    b = a -10
    c = (b/a)*100
    if (c > 100):
        d = 100
    else:
        d =0
    return(d)
        
########### Functional Programming
# Passing functions as inputs or outputs

def data_process(list1, mathop):
    return(mathop(list1))

l1 = [5,9,1,2,4]
data_process(l1, min)
data_process(l1, max)

airquality = pd.read_csv("data/airquality.csv")
airquality.apply(np.mean, axis = 0)

def max_min_diff(a1):
    return(max(a1) - min(a1))

max_min_diff(l1) # calling the function directly
airquality.apply(max_min_diff, axis = 0) # function passed as input to another function

""" following doesn't work
you can't pass multiple inputs to a function through
different channels in the context of functional programming
def max_min_diff2(a1, b):
    return(max(a1) - min(a1) - b)
airquality.apply(max_min_diff2(b), axis = 0)

"""
# above logic can be achieved by writing a for loop
def max_min_diff2(df, b):
    op = []
    for i in airquality.columns:
        op.append(max(airquality[i]) - min(airquality[i]) - b)
    return(op)
max_min_diff2(airquality, 10)

   
def custom_print(str1, str2, fn1, sep1 = "|"):
    fn1(str1,str2, sep = sep1)
custom_print("Hello","World", print)
custom_print("Hello","World", print, ";")


########### Assignment 
"""
Write a function which accepts a list/array/series as input and 
returns the difference between mean and median
"""
def mean_median_diff(a):
    return(np.mean(a) - np.median(a))

mean_median_diff([1,2,3,4,1000])

mean_median_diff2 = lambda b: np.mean(b) - np.median(b)
mean_median_diff2([1,2,3,4,1000])

"""
Write a function (max_var2_corresponding) which accepts a data frame (df) 
as input along with 2 column names (var1, var2) in the data frame. 
Calculate the maximum value in var1 column of df. 
Return the value of var2 corresponding to maximum value of var1
"""

def max_var2_corresponding(df, var1, var2):
    cond =df[var1] == df[var1].max()
    return(df.loc[cond,var2])

# above function in single line using lambda
max_var2_corresponding2 = lambda df, var1, var2: df.loc[df[var1] == df[var1].max(),var2]

# Test Case 1
math_score_array = np.array([95,67,88,45,84])
eng_score_array = np.array([78,67,45,39,67])
gender_array = np.array(["M","M","F","M","F"])
score_df = pd.DataFrame({
'Maths':math_score_array,
'English':eng_score_array,
'Gender':gender_array})
score_df.index = ["R1001","R1002","R1003","R1004","R1005"]


max_var2_corresponding(score_df,"Maths","English") #78
max_var2_corresponding(score_df,"English","Gender") #M

# Test Case 2
emp_details_dict = {
'Age': [25,32,28],
'Income': [1000,1600,1400]
}

emp_details = pd.DataFrame(emp_details_dict)
emp_details.index = ['Ram','Raj','Ravi']
max_var2_corresponding(emp_details,"Income","Age") #32

import numpy as np
# all functions with numpy will be imported
np.mean([5,6,7])
np.median([5,6,7])

import py_20190601_fns1 as fn1 # script should be present in the current working directory
# all functions in the script will be imported
fn1.print_with_excalamtion("Hello")
fn1.print_sum2(5,10)

### Selectively importing functions
from pandas import Series

s1 = Series([1,2,3,4,5], index = ["a","b","c","d","e"])

#DataFrame() dataframe function is not imported

from py_20190601_fns2 import calculate, polynom_l
calculate(5,10,"diff")
polynom_l(10)
#polynom_l2(5,10) this function is not imported

############################## Optimised Data Processing ##############

import pandas as pd
import numpy as np

airquality = pd.read_csv("data/airquality.csv")

np.random.seed(10)
df = pd.DataFrame({
        'Class_L1':['lkg','ukg','lkg','ukg','lkg','ukg','lkg','lkg'],
        'Class_L2':['aa','bb','bb','aa','aa','bb','bb','aa'],
        'Drawing':np.random.randint(1,100,8),
        'English':np.random.randint(1,100,8),
        'Maths':np.random.randint(1,100,8),
        'Rhymes':np.random.randint(1,100,8),
        'Stories':np.random.randint(1,100,8)})
## Calculate average of columns C till G
df["C"].mean()
for i in df.columns[2:]:
    print(df[i].mean())

######################## apply ################################################
# repetitively applying a function on rows/columns
    # axis = 0 for column wise operation
    # axis = 1 for row wise operation
df_s = df.iloc[:,2:]
df_s.apply(np.mean, axis = 0) # column wise mean
df_s.apply(np.mean, axis = 1) # row wise mean
df_s.apply(sum, axis = 0) # column wise sum
df_s.apply(np.median, axis = 1) # row wise median

#13. Calculate average values of Ozone, Solar, Wind and Temperature 
aq_s = airquality.iloc[:,:4]
aq_avg_values = aq_s.apply(np.mean, axis = 0)
aq_avg_values = airquality.iloc[:,:4].apply(np.mean, axis = 0) # slicing and applying can happen in 1 line too
airquality.mean() # this will do mean on all columns. this facility not available for all functions

######################### group by ############################################
# segment the data based on a categorical variable
df_lkg = df[df["Class_L1"] == "lkg"]
df_ukg = df[df["Class_L1"] == "ukg"]

df_classwise = df.groupby("Class_L1")
df_classwise.get_group("lkg")
df_classwise.get_group("ukg")
for i in df_classwise:
    print("********************************************")
    print(i)
    
df_sectionwise = df.groupby("Class_L2")
df_sectionwise.get_group("aa")
df_sectionwise.get_group("bb")

df_class_sectionwise =df.groupby(["Class_L1","Class_L2"]) # group based on 2 categorical variables
for i in df_class_sectionwise:
    print("***********************************")
    print(i)

################## aggregate ##################################################
    # do a repettive operation across groups

## applying a function across groups for a particular column
df_classwise["Maths"].agg(np.mean)
df_classwise["Drawing"].agg(np.median)
df_class_sectionwise["Rhymes"].agg(np.mean)

## applying a function across groups for a set of columns
df_classwise[["Maths","Rhymes","Drawing"]].agg(np.mean)

## applying multiple functions across groups
df_classwise["Maths"].agg([np.mean, np.median])
df_classwise["Rhymes"].agg([min, max])
df_classwise[["Maths","Rhymes","Drawing"]].agg([min, max])

## applying different function for different column
"""
 select average(Maths), median(Rhymes), min(Drawing), max(Drawing)
 from df
 group by ("Class_L1")
 """
df_classwise.agg({"Maths": np.mean,
                  "Rhymes": np.median,
                  "Drawing": [min, max]})


#14. Calculate month-wise average Ozone
aq_monthwise = airquality.groupby("Month")
aq_monthwise.get_group(5)
for i in aq_monthwise:
    print("***********************************")
    print(i)

aq_monthwise["Ozone"].agg(np.mean)

#15. Calculate month-wise average Ozone, Solar, Wind and Temperature
cols_needed = airquality.columns[:4]
aq_monthwise[cols_needed].agg(np.mean)

# Calculate month-wise average ozone, min and max temp
aq_monthwise.agg({"Ozone": np.mean,
                  "Temp": [min, max]})



	
